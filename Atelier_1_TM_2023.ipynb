{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K-VU9WD0yYn"
      },
      "source": [
        "# Atelier 1 : Techniques NLP de Base\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0v_j4N30yYo"
      },
      "source": [
        "# 1.\tObjectif\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eShrsBcZ0yYp"
      },
      "source": [
        "L’objectif de cet atelier est d’apprendre les tâches NLP les plus courantes à travers l’utilisation des bibliothèques nltk, scikitlearn et Spacy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om75wzht0yYp"
      },
      "source": [
        "# 2.\tOutils et environnement de travail\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Installer les packages nltk , Spacy, Scikitlearn et pywsd."
      ],
      "metadata": {
        "id": "KZIle89Q1Z3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "#nltk.download('punkt')"
      ],
      "metadata": {
        "id": "kokDFFNE1oz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnA0rBpZ0yYq"
      },
      "source": [
        "# 3.\tSegmentation (Tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYVYZdGf0yYq"
      },
      "source": [
        "La segmentation de texte et la tâche de subdivision du texte en petites unités qui seront plus simples à traiter et qu’on appelle tokens.\n",
        "La bibliothèque nltk offre à travers le module **tekenize** un certain nombre de tokinzers qui permettent de réaliser la segmentation du texte en fonction de la nature du problème : words tokenizer, regular-expression based tokenizer, sentences based tokinizers, etc. Ci-dessous une liste non exhaustive de quelques fonctions du module tokinize.\n",
        "\n",
        "* regexp_span_tokenize(text, regexp): Retourne les tokens de texte qui correspondent à l’expression régulière regexp\n",
        "\n",
        "* sent_tokenize(text[, language]):\tRetourne les phrases contenues dans le texte en utilisant le tokenizer PunktSentenceTokenizer.\n",
        "\n",
        "* word_tokenize(text[, language]:\tRetourne les mots contenus dans le texte en utilisant le tokenizer TreebankWordTokenizer avec PunktSentenceTokenizer.\n",
        "\n",
        "NLTK offre également un certain nombre de classes qui offrent des tokinizers plus avancés : BlanklineTokenizer, MWETokenizer, PunktSentenceTokenizer, TextTilingTokenizer, TweetTokenizer, etc.\n",
        "Ci-dessous deux exemples de tokenization à base de **sent_tokenize** et **word_tokenize**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IEwKpmf0yYq"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!\"\n",
        "\n",
        "sentences=sent_tokenize(data)\n",
        "print(\"sentences: \" , sentences)\n",
        "\n",
        "words=word_tokenize(data)\n",
        "print(\"Words: \",words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czBN4jcy0yYr"
      },
      "source": [
        "# 4.\tNettoyage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fsYyT9-0yYs"
      },
      "source": [
        "Le nettoyage des données texte joue un rôle très important dans l’amélioration des performances des opérations d’analyse et de découverte de paternes. Ça consiste à la suppression des termes non significatifs **\"Stop words\"**, comme par exemple « le », « la », « de », « du », « ce »… en français et « as » « the », « a », « an », « in » en anglais.  Ces termes qui sont présents fréquemment dans des documents texte peuvent influencer négativement sur la qualité des résultats d’analyse.\n",
        "Le nettoyage peut consister également à la supression des caratères de pontuation et des chaînes de caractères non alphabétiques.\n",
        "Ci-dessous le code qui permet de supprimer les stop words à partir d’un texte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fC3eZk-0yYs"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!\"\n",
        "word_tokens = [word.lower() for word in word_tokenize(data)]\n",
        "data_clean = [word for word in word_tokens if (not word in set(stopwords.words('english')) and  word.isalpha())]\n",
        "print(data_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF38c0q_0yYs"
      },
      "source": [
        "# 5.\tRacinisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGtnzZea0yYs"
      },
      "source": [
        "La racinisation (Stemming en anglais) permet de normaliser la représentation des mots contenus dans une expression texte en extrayant leurs racines. Ça permettra de supprimer toutes les redondances des mots ayant la même racine. Plusieurs **stemmers** sont offerts par nltk dont les plus utilisés sont : *PorterStemmer, LancasterStemmer, SnowballStemmer...*. Également, le module nltk.stem.snowball offre un certain nombre de stemmers personnalisés à chaque langue, comme par exemple :  *FrenchStemmer, ArabicStemmer*, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuBnt__I0yYt"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer,SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "#stemmer=SnowballStemmer('french')\n",
        "stemmer=PorterStemmer()\n",
        "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!\"\n",
        "\n",
        "word_tokens = [word.lower() for word in word_tokenize(data)]\n",
        "\n",
        "for i in range(len(word_tokens)):\n",
        "    words=[stemmer.stem(word) for word in word_tokens if (not word in set(stopwords.words('english')) and  word.isalpha())]\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8av6WXdk0yYt"
      },
      "source": [
        "# 6.\tLemmatisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wIpQHvG0yYt"
      },
      "source": [
        "A la différence de la racisation qui fournit souvent une représentation non significative et incomplète des mots, la lemmatisation permet d’obtenir les **formes canoniques** des mots contenus dans une expression texte. Ainsi, au lieu de supprimer juste les suffixes et les préfixes des mots pour obtenir leurs racines, la lemmatisation réalise une **analyse morphologique** des mots afin d’extraire leurs formats canoniques.\n",
        "nltk offre le lemmatizer  *WordNetLemmatizer* pour la réalisation des opérations de lemmatisation, mais uniquement pour l’anglais. pour d'autre langues on peut recourir à la bibliotheque **SpaCy**.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQeheHd10yYt"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "lemmmatizer=WordNetLemmatizer()\n",
        "data1=\"Le big data  « grosses données » en anglais,les mégadonnées ou les données massives, \" \\\n",
        "     \"désigne les ressources d’informations dont les caractéristiques en termes de volume,\" \\\n",
        "     \" de vélocité et de variété imposent l’utilisation de technologies et de méthodes analytiques \" \\\n",
        "     \"particulières pour générer de la valeur, et qui dépassent en général les capacités \" \\\n",
        "     \"d'une seule et unique machine et nécessitent des traitements parallélisés\"\n",
        "data2=\"Big data is a field that treats ways to analyze, systematically extract information from,\" \\\n",
        "     \" or otherwise deal with data sets that are too large or complex to be dealt with by traditional\" \\\n",
        "     \" data-processing application software. Data with many cases (rows) offer greater statistical power,\" \\\n",
        "     \" while data with higher complexity (more attributes or columns) \" \\\n",
        "     \"may lead to a higher false discovery rate. \"\n",
        "\n",
        "words1 = word_tokenize(data1)\n",
        "words1 = [lemmmatizer.lemmatize(word.lower()) for word in words1 if(not word in set(stopwords.words('french')) and  word.isalpha())]\n",
        "print(words1)\n",
        "\n",
        "words2 = word_tokenize(data2)\n",
        "words2 = [lemmmatizer.lemmatize(word.lower()) for word in words2 if(not word in set(stopwords.words('english')) and  word.isalpha())]\n",
        "print(words2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7h4x5d10yYt"
      },
      "source": [
        "#7.\tPOS-Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjcN_uWE0yYt"
      },
      "source": [
        "Le pos-tagging permet de réaliser une analyse lexicale d’une expression texte selon les règles de la grammaire. Les différentes unités seront dotées d’une annotation permettant de savoir le rôle grammatical de chaque mot dans l’expression. Les annotations les plus courante sont (DT : Determiner, NN : noun , JJ : adjective,  RB: adverb, VB : verb,  PRP : Personal Pronoun…).\n",
        "\n",
        "NLTK offre une panoplie de taggers pour le pos-taggin qui recoivent une liste de tokens et leurs attribuent automatiquement  des tags en se basant sur des corpus d'apprentisgae.  \n",
        "\n",
        "Par defaut la methode *pos_tag* offre un pos_tagging standard (Recommandé) pour l'anglais et cela en se bsant sur le tagset *\"Penn Treebank\"*:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLV-XP6_0yYu"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!. \"\n",
        "words=word_tokenize(data)\n",
        "print(nltk.pos_tag(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans le cas d'un document qui se compose de plusieurs phrases, il sera preferable d'utliser pos_tag_sents."
      ],
      "metadata": {
        "id": "JyD2EoJB4kEJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTdsprzi0yYu"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!. \"\n",
        "\n",
        "\n",
        "sentences=sent_tokenize(data)\n",
        "\n",
        "list=[]\n",
        "for sentence in sentences:\n",
        "    list.append(word_tokenize(sentence))\n",
        "\n",
        "print(nltk.pos_tag_sents(list))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UnigramTagger permet d'attribuer aux mots leurs tags les plus frequents par rapport à un corpus d'apprentissage.   "
      ],
      "metadata": {
        "id": "UISMSYrw4sHt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP_6-5wg0yYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc3c72f-ce84-4dd4-fac2-f62b987b1d82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8121200039868434\n",
            "[('Hello', None), (',', ','), ('i', None), ('am', 'BEM'), ('very', 'QL'), ('happy', 'JJ'), ('to', 'TO'), ('meet', 'VB'), ('you', 'PPSS'), ('.', '.'), ('I', 'PPSS'), ('created', 'VBN'), ('this', 'DT'), ('course', 'NN'), ('for', 'IN'), ('you', 'PPSS'), ('.', '.'), ('Good', 'JJ-TL'), ('by', 'IN'), ('!', '.'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-5e23a81f11d7>:12: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  print(unigram_tagger.evaluate(test_sents))\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "from nltk.tag import UnigramTagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
        "size = int(len(brown_tagged_sents) * 0.9)\n",
        "train_sents = brown_tagged_sents[:size]\n",
        "test_sents = brown_tagged_sents[size:]\n",
        "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
        "print(unigram_tagger.evaluate(test_sents))\n",
        "\n",
        "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!. \"\n",
        "print(unigram_tagger.tag(word_tokenize(data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sSLwHFo0yYu"
      },
      "source": [
        "le modèle n-gram est une generalisation de l'unigram qui cnsidère également le contexte où apparait le mot en considerant les tags des n-1 mots precedents.\n",
        "\n",
        "bigram tagger est un exemple generateur pos-tagging n-gram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqVvBkCw0yYu"
      },
      "outputs": [],
      "source": [
        "brown_tagged_sents = brown.tagged_sents()\n",
        "\n",
        "size = int(len(brown_tagged_sents) * 0.9)\n",
        "train_sents = brown_tagged_sents[:size]\n",
        "test_sents = brown_tagged_sents[size:]\n",
        "\n",
        "bigram_tagger = nltk.BigramTagger(train_sents)\n",
        "\n",
        "print(bigram_tagger.evaluate(test_sents))\n",
        "\n",
        "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!\"\n",
        "print(bigram_tagger.tag(word_tokenize(data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LYzZb5K0yYu"
      },
      "source": [
        "On t combiner plusieurs taggers en les executant d'une maniere sequentielle comme montré dans l'exemple c-dessous:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQTiOflB0yYv"
      },
      "outputs": [],
      "source": [
        "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
        "\n",
        "size = int(len(brown_tagged_sents) * 0.9)\n",
        "train_sents = brown_tagged_sents[:size]\n",
        "test_sents = brown_tagged_sents[size:]\n",
        "\n",
        "t0 = nltk.DefaultTagger('NN')\n",
        "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
        "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
        "print(t2.evaluate(test_sents))\n",
        "\n",
        "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!\"\n",
        "print(t2.tag(word_tokenize(data)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX_RmuuZ0yYv"
      },
      "source": [
        "Pour le moment la package nltk ne permet de faire le pos-tagging que pour l’anglais et le russe à l’aide du modèle « averaged_perceptron_tagger ».\n",
        "StanfordPOSTagguer permet faire du pos-tagging pour d’autre langues comme le français et l’arabe. Il suffit de télécharger les differents librairies nécessaires (https://nlp.stanford.edu/software/tagger.shtml) et utiliser celles qui correspondent à la langue comme présenté dans l’exemple ci-dessous."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip stanford-tagger-4.2.0"
      ],
      "metadata": {
        "id": "z9HoPuMa7FHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsfQiRCy0yYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de278d8-462f-4e6b-ab00-6e80ba036ff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Le', 'DET'), ('big', 'ADJ'), ('data', 'NOUN'), ('«', 'PUNCT'), ('grosses', 'ADJ'), ('données', 'NOUN'), ('»', 'PUNCT'), ('en', 'ADP'), ('anglais', 'NOUN'), (',', 'PUNCT'), ('les', 'DET'), ('mégadonnées', 'NOUN'), ('ou', 'CCONJ'), ('les', 'DET'), ('données', 'NOUN'), ('massives', 'ADJ'), (',', 'PUNCT'), ('désigne', 'VERB'), ('les', 'DET'), ('ressources', 'NOUN'), ('d', 'ADP'), ('’', 'NUM'), ('informations', 'NOUN'), ('dont', 'PRON'), ('les', 'DET'), ('caractéristiques', 'NOUN'), ('en', 'ADP'), ('termes', 'NOUN'), ('de', 'ADP'), ('volume', 'NOUN'), (',', 'PUNCT'), ('de', 'ADP'), ('vélocité', 'NOUN'), ('et', 'CCONJ'), ('de', 'ADP'), ('variété', 'NOUN'), ('imposent', 'VERB'), ('l', 'DET'), ('’', 'PUNCT'), ('utilisation', 'NOUN'), ('de', 'ADP'), ('technologies', 'NOUN'), ('et', 'CCONJ'), ('de', 'ADP'), ('méthodes', 'NOUN'), ('analytiques', 'ADJ'), ('particulières', 'ADJ'), ('pour', 'ADP'), ('générer', 'VERB'), ('de', 'ADP'), ('la', 'DET'), ('valeur', 'NOUN'), (',', 'PUNCT'), ('et', 'CCONJ'), ('qui', 'PRON'), ('dépassent', 'VERB'), ('en', 'ADP'), ('général', 'NOUN'), ('les', 'DET'), ('capacités', 'NOUN'), (\"d'une\", 'ADJ'), ('seule', 'ADJ'), ('et', 'CCONJ'), ('unique', 'ADJ'), ('machine', 'NOUN'), ('et', 'CCONJ'), ('nécessitent', 'VERB'), ('des', 'DET'), ('traitements', 'NOUN'), ('parallélisés', 'VERB')]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.tag.stanford import StanfordPOSTagger\n",
        "\n",
        "data=\"Le big data  « grosses données » en anglais,les mégadonnées ou les données massives, \" \\\n",
        "     \"désigne les ressources d’informations dont les caractéristiques en termes de volume,\" \\\n",
        "     \" de vélocité et de variété imposent l’utilisation de technologies et de méthodes analytiques \" \\\n",
        "     \"particulières pour générer de la valeur, et qui dépassent en général les capacités \" \\\n",
        "     \"d'une seule et unique machine et nécessitent des traitements parallélisés\"\n",
        "root=\"stanford-postagger-full-2020-11-17\"\n",
        "stf = StanfordPOSTagger(root+'/models/french-ud.tagger',root+\"/stanford-postagger.jar\",encoding='utf8')\n",
        "tokens = nltk.word_tokenize(data)\n",
        "print(stf.tag(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42NQzHRr0yYv"
      },
      "source": [
        "#8.\tAnalyse Sémantique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmxWc7_E0yYv"
      },
      "source": [
        "Un mot peut avoir plusieurs significations selon son contexte (les mots voisins et le rôle grammaticale). Par exemple, le mot anglais « break » possède 75 sens. Chose qui montre l’importance de la désambiguïsation lors de l’analyse d’un texte. Ci-dessous un extrait de la récupération des différents sens du mot « break » avec leurs annotations grammaticales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzNoBhDq0yYv"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "for synset in wordnet.synsets('break'):\n",
        "    print(\">>>\",synset.definition())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCHTAEpW0yYv"
      },
      "source": [
        "Pour un synset bien determiné on peut recuperer la liste des termes qui partage le même sens (La même description du sens):\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCl3w4zU0yYw"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "seynsets= wordnet.synsets('break')\n",
        "\n",
        "for synset in seynsets:\n",
        "    print(synset.lemmas())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXk0iyGe0yYw"
      },
      "source": [
        "On peut même récuperer le terme correspedant dans une autre langue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wS38aezy0yYw"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "seynsets= wordnet.synsets('break')\n",
        "\n",
        "#Francais\n",
        "for synset in seynsets:\n",
        "    print(synset.lemmas(lang='fra'))\n",
        "\n",
        "#Arabe\n",
        "for synset in seynsets:\n",
        "    print(synset.lemmas(lang='arb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28aLdZ7o0yYw"
      },
      "source": [
        "pour la liste des langues dsiponibles executer: sorted(wn.langs())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLBiK1XA0yYw"
      },
      "source": [
        "La bibliothèque nltk offre à travers le module **wsd** la possibilité de détecter le sens d’un mot en fonction de son contexte. A cette fin, l’algorithme **Lesk** est utilisé pour réaliser une désambiguïsation du sens d’un mot en retournant le sens qui a permis d’avoir le plus grand nombre de termes en intersection avec le contexte du mot pour lequel on est en train de chercher le sens exact. L’algorithme ne retourne aucun sens s’il n’arrive pas à réaliser la désambiguïsation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIRvjkce0yYx"
      },
      "outputs": [],
      "source": [
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "context= word_tokenize(\"I've just finished the first step of the competition. I need a little break to catch my breath\")\n",
        "print(lesk(context, 'break','n').definition())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bud4f9M-0yYx"
      },
      "source": [
        "L’exemple ci-dessus montre que l’algorithme n’est pas assez performant. D’autres algorithmes peuvent être utilisés en se basant sur les bibliothèques baseline, pywsd ou spaCy. Ci-dessous un autre exemple avec la bibliotheque pywsd.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrQi6KqJ0yYx"
      },
      "outputs": [],
      "source": [
        "!pip install pywsd\n",
        "# pip install -U pywsd\n",
        "#pip install wn==0.0.22\n",
        "from pywsd.lesk import simple_lesk\n",
        "sent = \"I've just finished the first step of the competition. I need a little break to catch my breath\"\n",
        "ambiguous = 'break'\n",
        "answer = simple_lesk(sent, ambiguous, pos='n')\n",
        "print (answer)\n",
        "print (answer.definition())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpU8Bt8E0yYx"
      },
      "source": [
        "#9.\tAnalyse syntaxique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if-fxqRf0yYx"
      },
      "source": [
        "L'objectif de cette section est d'analyser la structure grammaticale des phrases au lieu de se focaliser d'une manière individuelle sur les mots les composant. Nous nous contenetant dans un premier temps de l'approche grammaticale pour realiser l'inference de la structure arborescente d'une phrase.\n",
        "\n",
        "Il existe plusieurs bibliotehques permettant de reéaliser cette tâche. ci-dessous deux exemples avec les bibliotheque stanford et spacy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7m4o5uY0yYx"
      },
      "source": [
        "### Stanford"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip stanford-parser-4.2.0.zip"
      ],
      "metadata": {
        "id": "CbJzU54t97af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er2VWp3E0yYy"
      },
      "outputs": [],
      "source": [
        "#import os\n",
        "from nltk.parse import stanford\n",
        "!os.environ['STANFORD_PARSER'] = 'stanford-parser-full-2020-11-17'\n",
        "!os.environ['STANFORD_MODELS'] = 'stanford-parser-full-2020-11-17'\n",
        "\n",
        "parser = stanford.StanfordParser(model_path=\"stanford-parser-full-2020-11-17/stanford-parser-4.2.0-models/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
        "sentences = parser.raw_parse_sents((\"Hello, My name is Melroy.\", \"What is your name?\"))\n",
        "\n",
        "\n",
        "#Formatted\n",
        "\n",
        "for line in sentences:\n",
        "    for sentence in line:\n",
        "        print(sentence)\n",
        "\n",
        "\n",
        "'''\n",
        "# GUI\n",
        "for line in sentences:\n",
        "    for sentence in line:\n",
        "        sentence.draw()\n",
        "\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjDbbhkW0yYy"
      },
      "source": [
        "### SpaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRMSot9c0yYy"
      },
      "source": [
        "spaCy une bibliothèque de la NLP qui est très puissante (elle est orientée production, non uniquement pour la recherche ou l’apprentissage de la NLP), totalement écrite python, gratuite et libre.\n",
        "Par défaut, lorsqu’on fait appel au module **nlp** de spaCy, les opérations suivantes sont exécutées : Segmentation, pos-tagging, analyse syntaxique, NER (Named Entity Recognition), etc.  Un objet Doc est retourné à l’issue de toutes les opérations et qui encapsule tous les resultats de l’analyse.\n",
        "\n",
        "L’exemple ci-dessous montre comment extraire les différentes informations à partir d’un objet Doc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aisuYqTT0yYy"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download fr_core_news_sm\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('fr_core_news_sm')\n",
        "doc = nlp(\"Le big data  « grosses données » en anglais,les mégadonnées ou les données massives, \" \\\n",
        "     \"désigne les ressources d’informations dont les caractéristiques en termes de volume,\" \\\n",
        "     \" de vélocité et de variété imposent l’utilisation de technologies et de méthodes analytiques \" \\\n",
        "     \"particulières pour générer de la valeur, et qui dépassent en général les capacités \" \\\n",
        "     \"d'une seule et unique machine et nécessitent des traitements parallélisés\")\n",
        "\n",
        "for token in doc:\n",
        "    print(\"/token:\",token.text, \"/lemma:\",token.lemma_, token.shape_, token.is_alpha, token.is_stop,\"/POS:\", token.tag_, \"/PARS:\", token.head, token.dep_, \"/NER:\", token.ent_iob_, token.ent_type_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXv100B-0yYy"
      },
      "source": [
        "Pour récuperer L'arbre de dependance syntaxique evec spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFFj4X6p0yYy"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Hello, My name is Melroy. What is your name?\")\n",
        "\n",
        "#Visualisation 1\n",
        "print (\"{:<15} | {:<8} | {:<15} |{:<10} | {:<20}\".format('Token','Relation','Head','POS', 'Children'))\n",
        "print (\"-\" * 70)\n",
        "for token in doc:\n",
        "  # Print the token, dependency nature, head and all dependents of the token\n",
        "  print (\"{:<15} | {:<8} | {:<15} |{:<10} | {:<20}\"\n",
        "         .format(str(token.text), str(token.dep_), str(token.head.text), str(token.head.pos_), str([child for child in token.children])))\n",
        "\n",
        "\n",
        "#Visualisation 2 (graphique)\n",
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 120})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AONamkg10yYy"
      },
      "source": [
        "# 10.\tExercices :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnE8UK8y0yYy"
      },
      "source": [
        "## 10.1 Exercice 1: Traduction automatique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O66-7G5p0yYz"
      },
      "source": [
        "On desire assister l'utilisateur pendant la traduction de l'anglais vers le francais.\n",
        "\n",
        "* Constituer le contexte du document en recuperant tous les termes sigfificatifs\n",
        "* Découper le texte en des phrases simples et recuperer les tags de leurs mots.\n",
        "* Pour chaque phrase récuperer le sens exacte de chaque terme en se basant sur leurs Tags et leur contexts\n",
        "* Récuperer les termes correspendant en francais\n",
        "* Pour chaque phrase afficher à l'utilisateur les propostions de traduction pour les nom, les adjectifs et les verbes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDlaHpvh0yYz"
      },
      "source": [
        "## 10.2 Exercice 2: Detection du plagiarisme\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3kofjN80yYz"
      },
      "source": [
        "L’objectif de cet exercice est de détecter le pélagianisme à partir de wikipedia pendant la préparation des réponses à un certain nombre de questions sur des connaissances en informatique. Le dataset utilisé peut-être récupéré à partir du lien suivant :Cliquer <a href=\"https://ir.shef.ac.uk/cloughie/resources/plagiarism_corpus.html#Download\" target=\"_blank\">ICI</a>\n",
        "\n",
        "Pour ce faire, nous nous basant sur le calcul des similarités entre les réponses des candidats et les définitions exactes trouvées sur Wikipédia. Deux méthodes de calcul de similarité sont à utiliser, à savoir, la similarité syntaxique (orientée caractères) et la similarité sémantique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkLMpiqv0yYz"
      },
      "source": [
        "### Similarité Syntaxique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq0gFIsO0yYz"
      },
      "source": [
        "Pour la similarité  syntaxique entre des documents courts (des phrases) on peut recrorir à l'utilisation de l'un des algorithmes suivants:\n",
        "* Longest Common Sequence (LCS)\n",
        "* Set features\n",
        "* Word Order Similarity\n",
        "* n-gram sentences\n",
        "* Jaro-Winkler\n",
        "* ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBrn1Jo00yYz"
      },
      "source": [
        "* Recuperer le dataset du plagiarisme?\n",
        "* Réaliser les différentes tâches de prétraitement?\n",
        "* Calculer les similarités syntaxiques entre les réponses des étudiants et les définitions trouvées sur wikipedia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgTyUYL00yYz"
      },
      "source": [
        "### Similarité Sémantique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQZB-guC0yYz"
      },
      "source": [
        "WordNet est une base de données lexicale qui comporte des concepts (termes) classifiés et reliés les uns aux autres à travers des realtions semantiques\n",
        "\n",
        "La composante principale de wordNet est le synset (synonym set) tel que chacun contient plusieurs mots qui partagent le même sens (des lemmas). Egalement, un mot peut appartenir à plusieurs synsets à la foix.\n",
        "\n",
        "l'exemple suivant montre comment recuperer les synsets d'un mot et comment recuperer ses synonymes pour un sens particuliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMeuLT1K0yY0"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "computer_synsets = wn.synsets(\"computer\")\n",
        "print(\"Computer sens in wordNet:\")\n",
        "i=0;\n",
        "for sense in computer_synsets:\n",
        "    print(\" \\t Sens :\", i)\n",
        "    print(\" \\t\\t Sens definition: \"+sense.definition())\n",
        "    lemmas = [l.name() for l in sense.lemmas()]\n",
        "    print(\"\\t\\t Lemmas for sense :\" +str(lemmas))\n",
        "    i=i+1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJQURAq40yY0"
      },
      "source": [
        "Pour la similarité sémantique, la bibliothèque nltk et à travers le module wordnet permet de mesurer la distance ou la similarité sémantique entre les sens des mots. Ainsi, en récupérant les sens synset1 et synset2 de deux mots quelconques plusieurs façons sont possibles pour calculer leur similarité:\n",
        "\n",
        "* synset1.path_similarity(synset2) : retourne leur ordre de similarité sous forme d’une valeur numérique entre 0 et 1 en se basant sur le plus court chemin qui relie les deux sens dans l’arborescence de wordnet.\n",
        "* synset1.lch_similarity(synset2): qui se base sur l’algorithme Leacock-Chodorow\n",
        "* Synset1.wup_similarity(synset2): qui se base sur l’algorithme Wu-Palmer\n",
        "* synset1.res_similarity(synset2, ic): qui se base sur l’algorithme Resnik:\n",
        "* synset1.jcn_similarity(synset2, ic): qui se base sur l’algorithme Jiang-Conrath\n",
        "* synset1.lin_similarity(synset2, ic): qui se base sur l’algorithme Lin\n",
        "\n",
        "l'exemple suivant montre comment calculer les similarité entres les sens des termes computer et device en se basant sur les metriques Leacock-Chodorow et Wu-Palmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQnsr0270yY0"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "computer_synsets = wn.synsets(\"computer\")\n",
        "device_synsets = wn.synsets(\"device\")\n",
        "lch=[]\n",
        "wup=[]\n",
        "\n",
        "\n",
        "for s1 in computer_synsets:\n",
        "    for s2 in device_synsets:\n",
        "        lch.append(s1.lch_similarity(s2))\n",
        "        wup.append(s1.wup_similarity(s2))\n",
        "\n",
        "pd.DataFrame([lch,wup],[\"lch\",\"wup\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSW-7rBk0yY0"
      },
      "source": [
        "Souvent on aura besoin de recuperer les sens exactes des termes dans leurs contextes afin mesurer leurs similarité d'une manière plus precise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8T7BV-k0yY0"
      },
      "outputs": [],
      "source": [
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "def WSD(word, doc):\n",
        "    context= word_tokenize(doc)\n",
        "    sens=lesk(context, word)\n",
        "    return sens\n",
        "\n",
        "doc1='Computer science is the study of computers and computing concepts. It includes both hardware and software, as well as networking and the Internet'\n",
        "doc2='Computer science is the science that deals with the theory and methods of processing information in digital computers, the design of computer hardware and software, and the applications of computers.'\n",
        "\n",
        "\n",
        "print(WSD(\"Computer\", doc1).definition())\n",
        "print(WSD(\"Computer\", doc2).definition())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfYvxSDE0yY0"
      },
      "source": [
        "Pour calculer la distance semantique entre deux documents, on aura besoin de calculer les similarités semantiques entre leurs mots deux à deux ou utiliser par example la distance de Hausdorff ou l'indic de Jaccard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pXtXU360yY0"
      },
      "source": [
        "* Defnir la fonction SemanticDistanceDocs(doc1,doc2) qui permet de calculer la distance semantique totale entre deux documents texte?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIciQT0Y0yY0"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "doc1='Computer science is the study of computers and computing concepts. It includes both hardware and software, as well as networking and the Internet'\n",
        "doc2='Computer science is the science that deals with the theory and methods of processing information in digital computers, the design of computer hardware and software, and the applications of computers.'\n",
        "\n",
        "def SemanticDistanceDocs(doc1,doc2):\n",
        "    sensDoc1=[WSD(word, doc1) for word in word_tokenize(doc1)]\n",
        "    sensDoc2=[WSD(word, doc2) for word in word_tokenize(doc2)]\n",
        "    '''A completer'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n8GPch00yY1"
      },
      "source": [
        "* Calculer les similarités syntaxiques entre les réponses des étudiants et les définitions trouvées sur wikipedia?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "U0v_j4N30yYo",
        "Om75wzht0yYp",
        "KnA0rBpZ0yYq",
        "czBN4jcy0yYr",
        "YF38c0q_0yYs",
        "8av6WXdk0yYt",
        "x7h4x5d10yYt",
        "42NQzHRr0yYv",
        "HpU8Bt8E0yYx",
        "AONamkg10yYy"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}